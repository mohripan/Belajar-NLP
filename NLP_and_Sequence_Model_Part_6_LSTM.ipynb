{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE13i5TXfV19n57d8+nJSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohripan/Belajar-NLP/blob/main/NLP_and_Sequence_Model_Part_6_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cv0HKHJ0-5_m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data\n",
        "sentences = ['Believe in yourself and your abilities.',\n",
        "             'Never give up on your dreams, no matter how big or small they may seem.',\n",
        "             'Keep going, even when things get tough.',\n",
        "             'A flock of flamingos.',\n",
        "             'An octopus eight arms.',\n",
        "             'A popsicle on a hot day.']\n",
        "\n",
        "labels = [1, 1, 1, 0, 0, 0]"
      ],
      "metadata": {
        "id": "YyZbsk4H_KV5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "  doc = nlp(sentence)\n",
        "  tokenized_sentences.append([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "id": "WWE_Ad8Q_MK7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training word2vec\n",
        "w2v = Word2Vec(tokenized_sentences, vector_size = 50, min_count = 1, window = 5)"
      ],
      "metadata": {
        "id": "sOqJP6y4_ZSb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Embeddings\n",
        "embeddings = []\n",
        "for sentence in tokenized_sentences:\n",
        "  embeddings.append([w2v.wv[word] for word in sentence])"
      ],
      "metadata": {
        "id": "5WLiq25L_gLO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of embeddings into tensor\n",
        "embeddings  = [torch.FloatTensor(sentence) for sentence in embeddings]\n",
        "\n",
        "# Padding sequences\n",
        "embeddings = nn.utils.rnn.pad_sequence(embeddings, batch_first = True)\n",
        "\n",
        "# Converting label to tensor\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "xEnxRxtR_nCl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogdwN70CAW62",
        "outputId": "f3ddf966-c438-4400-f076-eae9ac4661c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 17, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    lstm_out, _ = self.lstm(x)\n",
        "    last_out = lstm_out[:, -1, :]\n",
        "    out = self.fc(last_out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "cBZ1vxyF_0MU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Initialization\n",
        "model = LSTM(50, 32, 1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "-nFDb4aMAmdB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "for epoch in range(50):\n",
        "  model.zero_grad()\n",
        "  output = model(embeddings)\n",
        "  loss = criterion(output.squeeze(), labels.float())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcOCDA1fAvJJ",
        "outputId": "83ed45ab-e596-401e-efb5-031e342e0636"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.6970\n",
            "Epoch: 1 | Loss: 0.6940\n",
            "Epoch: 2 | Loss: 0.6924\n",
            "Epoch: 3 | Loss: 0.6920\n",
            "Epoch: 4 | Loss: 0.6921\n",
            "Epoch: 5 | Loss: 0.6915\n",
            "Epoch: 6 | Loss: 0.6907\n",
            "Epoch: 7 | Loss: 0.6900\n",
            "Epoch: 8 | Loss: 0.6895\n",
            "Epoch: 9 | Loss: 0.6889\n",
            "Epoch: 10 | Loss: 0.6882\n",
            "Epoch: 11 | Loss: 0.6872\n",
            "Epoch: 12 | Loss: 0.6859\n",
            "Epoch: 13 | Loss: 0.6843\n",
            "Epoch: 14 | Loss: 0.6824\n",
            "Epoch: 15 | Loss: 0.6799\n",
            "Epoch: 16 | Loss: 0.6759\n",
            "Epoch: 17 | Loss: 0.6707\n",
            "Epoch: 18 | Loss: 0.6594\n",
            "Epoch: 19 | Loss: 0.6474\n",
            "Epoch: 20 | Loss: 0.6627\n",
            "Epoch: 21 | Loss: 0.6404\n",
            "Epoch: 22 | Loss: 0.6339\n",
            "Epoch: 23 | Loss: 0.6293\n",
            "Epoch: 24 | Loss: 0.6147\n",
            "Epoch: 25 | Loss: 0.5584\n",
            "Epoch: 26 | Loss: 0.4606\n",
            "Epoch: 27 | Loss: 0.7264\n",
            "Epoch: 28 | Loss: 0.4784\n",
            "Epoch: 29 | Loss: 0.4522\n",
            "Epoch: 30 | Loss: 0.2923\n",
            "Epoch: 31 | Loss: 0.4316\n",
            "Epoch: 32 | Loss: 0.3557\n",
            "Epoch: 33 | Loss: 0.1698\n",
            "Epoch: 34 | Loss: 0.2131\n",
            "Epoch: 35 | Loss: 0.1080\n",
            "Epoch: 36 | Loss: 0.0607\n",
            "Epoch: 37 | Loss: 0.0520\n",
            "Epoch: 38 | Loss: 0.0668\n",
            "Epoch: 39 | Loss: 0.0436\n",
            "Epoch: 40 | Loss: 0.0336\n",
            "Epoch: 41 | Loss: 0.0228\n",
            "Epoch: 42 | Loss: 0.0159\n",
            "Epoch: 43 | Loss: 0.0125\n",
            "Epoch: 44 | Loss: 0.0107\n",
            "Epoch: 45 | Loss: 0.0094\n",
            "Epoch: 46 | Loss: 0.0085\n",
            "Epoch: 47 | Loss: 0.0076\n",
            "Epoch: 48 | Loss: 0.0069\n",
            "Epoch: 49 | Loss: 0.0062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5Iv8IgFA_RP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}